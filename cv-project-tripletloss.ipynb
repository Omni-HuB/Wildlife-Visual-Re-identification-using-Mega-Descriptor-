{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["%pip install wildlife-datasets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install wildlife-tools"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install timm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from itertools import chain\n","import timm\n","import pandas as pd\n","import torchvision.transforms as T\n","from torch.optim import SGD\n","\n","from wildlife_tools.data import WildlifeDataset, SplitMetadata\n","from wildlife_tools.train import ArcFaceLoss, BasicTrainer\n","\n","import timm\n","import numpy as np\n","from wildlife_datasets.datasets import MacaqueFaces\n","from wildlife_tools.data import WildlifeDataset\n","import torchvision.transforms as T\n","from wildlife_datasets import datasets, splits\n","from wildlife_tools.features import DeepFeatures\n","from wildlife_tools.similarity import CosineSimilarity\n","from wildlife_tools.inference import KnnClassifier\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","import timm\n","import itertools\n","from torch.optim import SGD\n","from wildlife_tools.train import ArcFaceLoss, BasicTrainer , TripletLoss"]},{"cell_type":"markdown","metadata":{},"source":["**IPanda50 Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# Download dataset (if not already downloaded)\n","datasets.IPanda50.get_data('../data/IPanda50')\n","# Load dataset metadata\n","metadata = datasets.IPanda50('../data/IPanda50')\n","transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n","dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n","\n","\n","# Download MegaDescriptor-T backbone from HuggingFace Hub\n","backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n","\n","# Arcface loss - needs backbone output size and number of classes.\n","objective = TripletLoss()\n","\n","# Optimize parameters in backbone and in objective using single optimizer.\n","params = itertools.chain(backbone.parameters(), objective.parameters())\n","optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n","\n","def print_epoch_loss(trainer, epoch_data):\n","    # This function will print the average loss at the end of each epoch\n","    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n","\n","\n","trainer = BasicTrainer(\n","    dataset=dataset,\n","    model=backbone,\n","    objective=objective,\n","    optimizer=optimizer,\n","    epochs=20,\n","    device='cuda',\n","    epoch_callback=print_epoch_loss\n",")\n","\n","trainer.train()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","\n","trainer.save(\"retrained_chks\", file_name=\"tripletloss_IPanda50_retrained_checkpoint.pth\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n","dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)\n","\n","# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n","extractor_P = DeepFeatures(backbone , device = 'cuda')\n","\n","query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["similarity_function = CosineSimilarity()\n","similarity_P = similarity_function(query_P, database_P)\n","print(similarity_P)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n","predictions_P = classifier_P(similarity_P['cosine'])\n","print(\"Predictions for 100 test Images:-\\n\",predictions_P)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n","print(\"Accuracy on IPanda50 data: {:.2f}%\".format(accuracy_P * 100))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","print(\"Precision:\", precision_P)\n","print(\"Recall:\", recall_P)\n","print(\"F1 Score:\", f1_P)"]},{"cell_type":"markdown","metadata":{},"source":["**MacaqueFaces Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download dataset (if not already downloaded)\n","datasets.MacaqueFaces.get_data('../data/MacaqueFaces')\n","# Load dataset metadata\n","metadata = datasets.MacaqueFaces('../data/MacaqueFaces')\n","transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n","dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n","\n","\n","# Download MegaDescriptor-T backbone from HuggingFace Hub\n","backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n","\n","# Arcface loss - needs backbone output size and number of classes.\n","objective = TripletLoss()\n","\n","# Optimize parameters in backbone and in objective using single optimizer.\n","params = itertools.chain(backbone.parameters(), objective.parameters())\n","optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n","\n","def print_epoch_loss(trainer, epoch_data):\n","    # This function will print the average loss at the end of each epoch\n","    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n","\n","\n","trainer = BasicTrainer(\n","    dataset=dataset,\n","    model=backbone,\n","    objective=objective,\n","    optimizer=optimizer,\n","    epochs=20,\n","    device='cuda',\n","    epoch_callback=print_epoch_loss\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","import torch\n","\n","trainer.save(\"retrained_chks\", file_name=\"tripletloss_MacaqueFaces_retrained_checkpoint.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n","dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)\n","\n","# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n","extractor_P = DeepFeatures(backbone , device = 'cuda')\n","\n","query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)\n","\n","similarity_function = CosineSimilarity()\n","similarity_P = similarity_function(query_P, database_P)\n","print(similarity_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n","predictions_P = classifier_P(similarity_P['cosine'])\n","print(\"Predictions for 100 test Images:-\\n\",predictions_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n","print(\"Accuracy on MacaqueFaces data: {:.2f}%\".format(accuracy_P * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","print(\"Precision:\", precision_P)\n","print(\"Recall:\", recall_P)\n","print(\"F1 Score:\", f1_P)"]},{"cell_type":"markdown","metadata":{},"source":["**LionData Dateset**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download dataset (if not already downloaded)\n","datasets.LionData.get_data('../data/LionData')\n","# Load dataset metadata\n","metadata = datasets.LionData('../data/LionData')\n","transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n","dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n","\n","\n","# Download MegaDescriptor-T backbone from HuggingFace Hub\n","backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n","\n","# Arcface loss - needs backbone output size and number of classes.\n","objective = TripletLoss()\n","\n","# Optimize parameters in backbone and in objective using single optimizer.\n","params = itertools.chain(backbone.parameters(), objective.parameters())\n","optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n","\n","def print_epoch_loss(trainer, epoch_data):\n","    # This function will print the average loss at the end of each epoch\n","    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n","\n","\n","trainer = BasicTrainer(\n","    dataset=dataset,\n","    model=backbone,\n","    objective=objective,\n","    optimizer=optimizer,\n","    epochs=20,\n","    device='cuda',\n","    epoch_callback=print_epoch_loss\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","\n","trainer.save(\"retrained_chks\", file_name=\"tripletloss_LionData_retrained_checkpoint.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n","dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)\n","\n","# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n","extractor_P = DeepFeatures(backbone , device = 'cuda')\n","\n","query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)\n","\n","\n","similarity_function = CosineSimilarity()\n","similarity_P = similarity_function(query_P, database_P)\n","print(similarity_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n","predictions_P = classifier_P(similarity_P['cosine'])\n","print(\"Predictions for 100 test Images:-\\n\",predictions_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n","print(\"Accuracy on LionData data: {:.2f}%\".format(accuracy_P * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","print(\"Precision:\", precision_P)\n","print(\"Recall:\", recall_P)\n","print(\"F1 Score:\", f1_P)"]},{"cell_type":"markdown","metadata":{},"source":["**NyalaDataSet**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download dataset (if not already downloaded)\n","datasets.NyalaData.get_data('../data/NyalaData')\n","# Load dataset metadata\n","metadata = datasets.NyalaData('../data/NyalaData')\n","transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n","dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n","\n","\n","# Download MegaDescriptor-T backbone from HuggingFace Hub\n","backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n","\n","# Arcface loss - needs backbone output size and number of classes.\n","objective = TripletLoss()\n","\n","# Optimize parameters in backbone and in objective using single optimizer.\n","params = itertools.chain(backbone.parameters(), objective.parameters())\n","optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n","\n","def print_epoch_loss(trainer, epoch_data):\n","    # This function will print the average loss at the end of each epoch\n","    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n","\n","\n","trainer = BasicTrainer(\n","    dataset=dataset,\n","    model=backbone,\n","    objective=objective,\n","    optimizer=optimizer,\n","    epochs=20,\n","    device='cuda',\n","    epoch_callback=print_epoch_loss\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","\n","trainer.save(\"retrained_chks\", file_name=\"tripletloss_LionData_retrained_checkpoint.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n","dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)\n","\n","# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n","extractor_P = DeepFeatures(backbone , device = 'cuda')\n","\n","query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)\n","\n","\n","similarity_function = CosineSimilarity()\n","similarity_P = similarity_function(query_P, database_P)\n","print(similarity_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n","predictions_P = classifier_P(similarity_P['cosine'])\n","print(\"Predictions for 100 test Images:-\\n\",predictions_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n","print(\"Accuracy on NyalaData data: {:.2f}%\".format(accuracy_P * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","print(\"Precision:\", precision_P)\n","print(\"Recall:\", recall_P)\n","print(\"F1 Score:\", f1_P)"]},{"cell_type":"markdown","metadata":{},"source":["**StripeSpotter Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download dataset (if not already downloaded)\n","datasets.StripeSpotter.get_data('../data/StripeSpotter')\n","# Load dataset metadata\n","metadata = datasets.StripeSpotter('../data/StripeSpotter')\n","transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n","dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n","\n","\n","# Download MegaDescriptor-T backbone from HuggingFace Hub\n","backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n","\n","# Arcface loss - needs backbone output size and number of classes.\n","objective = TripletLoss()\n","\n","# Optimize parameters in backbone and in objective using single optimizer.\n","params = itertools.chain(backbone.parameters(), objective.parameters())\n","optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n","\n","def print_epoch_loss(trainer, epoch_data):\n","    # This function will print the average loss at the end of each epoch\n","    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n","\n","\n","trainer = BasicTrainer(\n","    dataset=dataset,\n","    model=backbone,\n","    objective=objective,\n","    optimizer=optimizer,\n","    epochs=20,\n","    device='cuda',\n","    epoch_callback=print_epoch_loss\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","\n","trainer.save(\"retrained_chks\", file_name=\"tripletloss_LionData_retrained_checkpoint.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n","dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)\n","\n","# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n","extractor_P = DeepFeatures(backbone , device = 'cuda')\n","\n","query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)\n","\n","\n","similarity_function = CosineSimilarity()\n","similarity_P = similarity_function(query_P, database_P)\n","print(similarity_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n","predictions_P = classifier_P(similarity_P['cosine'])\n","print(\"Predictions for 100 test Images:-\\n\",predictions_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n","print(\"Accuracy on StripeSpotter data: {:.2f}%\".format(accuracy_P * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","print(\"Precision:\", precision_P)\n","print(\"Recall:\", recall_P)\n","print(\"F1 Score:\", f1_P)"]},{"cell_type":"markdown","metadata":{},"source":["**CZoo Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download dataset (if not already downloaded)\n","datasets.CZoo.get_data('../data/CZoo')\n","# Load dataset metadata\n","metadata = datasets.CZoo('../data/CZoo')\n","transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n","dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n","\n","\n","# Download MegaDescriptor-T backbone from HuggingFace Hub\n","backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n","\n","# Arcface loss - needs backbone output size and number of classes.\n","objective = TripletLoss()\n","\n","# Optimize parameters in backbone and in objective using single optimizer.\n","params = itertools.chain(backbone.parameters(), objective.parameters())\n","optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n","\n","def print_epoch_loss(trainer, epoch_data):\n","    # This function will print the average loss at the end of each epoch\n","    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n","\n","\n","trainer = BasicTrainer(\n","    dataset=dataset,\n","    model=backbone,\n","    objective=objective,\n","    optimizer=optimizer,\n","    epochs=20,\n","    device='cuda',\n","    epoch_callback=print_epoch_loss\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","\n","trainer.save(\"retrained_chks\", file_name=\"tripletloss_LionData_retrained_checkpoint.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n","dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)\n","\n","# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n","extractor_P = DeepFeatures(backbone , device = 'cuda')\n","\n","query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)\n","\n","\n","similarity_function = CosineSimilarity()\n","similarity_P = similarity_function(query_P, database_P)\n","print(similarity_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n","predictions_P = classifier_P(similarity_P['cosine'])\n","print(\"Predictions for 100 test Images:-\\n\",predictions_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n","print(\"Accuracy on CZoo data: {:.2f}%\".format(accuracy_P * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","print(\"Precision:\", precision_P)\n","print(\"Recall:\", recall_P)\n","print(\"F1 Score:\", f1_P)"]},{"cell_type":"markdown","metadata":{},"source":["**CowDataset**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download dataset (if not already downloaded)\n","datasets.CowDataset.get_data('../data/CowDataset')\n","# Load dataset metadata\n","metadata = datasets.CowDataset('../data/CowDataset')\n","transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n","dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n","\n","\n","# Download MegaDescriptor-T backbone from HuggingFace Hub\n","backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n","\n","# Arcface loss - needs backbone output size and number of classes.\n","objective = TripletLoss()\n","\n","# Optimize parameters in backbone and in objective using single optimizer.\n","params = itertools.chain(backbone.parameters(), objective.parameters())\n","optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n","\n","def print_epoch_loss(trainer, epoch_data):\n","    # This function will print the average loss at the end of each epoch\n","    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n","\n","\n","trainer = BasicTrainer(\n","    dataset=dataset,\n","    model=backbone,\n","    objective=objective,\n","    optimizer=optimizer,\n","    epochs=20,\n","    device='cuda',\n","    epoch_callback=print_epoch_loss\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","\n","trainer.save(\"retrained_chks\", file_name=\"tripletloss_LionData_retrained_checkpoint.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n","dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)\n","\n","# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n","extractor_P = DeepFeatures(backbone , device = 'cuda')\n","\n","query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)\n","\n","\n","similarity_function = CosineSimilarity()\n","similarity_P = similarity_function(query_P, database_P)\n","print(similarity_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n","predictions_P = classifier_P(similarity_P['cosine'])\n","print(\"Predictions for 100 test Images:-\\n\",predictions_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n","print(\"Accuracy on CowDataset data: {:.2f}%\".format(accuracy_P * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","print(\"Precision:\", precision_P)\n","print(\"Recall:\", recall_P)\n","print(\"F1 Score:\", f1_P)"]},{"cell_type":"markdown","metadata":{},"source":["# **ON New Unseen Datasets**"]},{"cell_type":"markdown","metadata":{},"source":["**DogFaceNet Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download dataset (if not already downloaded)\n","datasets.DogFaceNet.get_data('../data/DogFaceNet')\n","\n","# Load dataset metadata\n","metadata = datasets.DogFaceNet('../data/DogFaceNet')\n","transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n","dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n","\n","\n","# Download MegaDescriptor-T backbone from HuggingFace Hub\n","backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n","\n","# Arcface loss - needs backbone output size and number of classes.\n","objective = TripletLoss()\n","\n","# Optimize parameters in backbone and in objective using single optimizer.\n","params = itertools.chain(backbone.parameters(), objective.parameters())\n","optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n","\n","def print_epoch_loss(trainer, epoch_data):\n","    # This function will print the average loss at the end of each epoch\n","    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n","\n","\n","trainer = BasicTrainer(\n","    dataset=dataset,\n","    model=backbone,\n","    objective=objective,\n","    optimizer=optimizer,\n","    epochs=20,\n","    device='cuda',\n","    epoch_callback=print_epoch_loss\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","\n","trainer.save(\"retrained_chks\", file_name=\"tripletloss_DogFaceNet_retrained_checkpoint.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n","dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)\n","\n","# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n","extractor_P = DeepFeatures(backbone , device = 'cuda')\n","\n","query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["similarity_function = CosineSimilarity()\n","similarity_P = similarity_function(query_P, database_P)\n","print(similarity_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n","predictions_P = classifier_P(similarity_P['cosine'])\n","print(\"Predictions for 100 test Images:-\\n\",predictions_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n","print(\"Accuracy on DogFaceNet data: {:.2f}%\".format(accuracy_P * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","print(\"Precision:\", precision_P)\n","print(\"Recall:\", recall_P)\n","print(\"F1 Score:\", f1_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
