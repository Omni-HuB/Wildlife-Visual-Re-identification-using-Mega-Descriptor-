{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install wildlife-datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install wildlife-tools","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install timm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from itertools import chain\nimport timm\nimport pandas as pd\nimport torchvision.transforms as T\nfrom torch.optim import SGD\n\nfrom wildlife_tools.data import WildlifeDataset, SplitMetadata\nfrom wildlife_tools.train import ArcFaceLoss, BasicTrainer\n\nimport timm\nimport numpy as np\nfrom wildlife_datasets.datasets import MacaqueFaces\nfrom wildlife_tools.data import WildlifeDataset\nimport torchvision.transforms as T\nfrom wildlife_datasets import datasets, splits\nfrom wildlife_tools.features import DeepFeatures\nfrom wildlife_tools.similarity import CosineSimilarity\nfrom wildlife_tools.inference import KnnClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nimport timm\nimport itertools\nfrom torch.optim import SGD\nfrom wildlife_tools.train import ArcFaceLoss, BasicTrainer , TripletLoss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**IPanda50 Dataset**","metadata":{}},{"cell_type":"code","source":"\n\n# Download dataset (if not already downloaded)\ndatasets.IPanda50.get_data('../data/IPanda50')\n# Load dataset metadata\nmetadata = datasets.IPanda50('../data/IPanda50')\ntransform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\ndataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n\n\n# Download MegaDescriptor-T backbone from HuggingFace Hub\nbackbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n\n# Arcface loss - needs backbone output size and number of classes.\nobjective = TripletLoss()\n\n# Optimize parameters in backbone and in objective using single optimizer.\nparams = itertools.chain(backbone.parameters(), objective.parameters())\noptimizer = SGD(params=params, lr=0.001, momentum=0.9)\n\ndef print_epoch_loss(trainer, epoch_data):\n    # This function will print the average loss at the end of each epoch\n    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n\n\ntrainer = BasicTrainer(\n    dataset=dataset,\n    model=backbone,\n    objective=objective,\n    optimizer=optimizer,\n    epochs=20,\n    device='cuda',\n    epoch_callback=print_epoch_loss\n)\n\ntrainer.train()\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ntrainer.save(\"retrained_chks\", file_name=\"tripletloss_IPanda50_retrained_checkpoint.pth\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\ndataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)\n\n# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\nextractor_P = DeepFeatures(trainer.model , device = 'cuda')\n\nquery_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity_function = CosineSimilarity()\nsimilarity_P = similarity_function(query_P, database_P)\nprint(similarity_P)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\npredictions_P = classifier_P(similarity_P['cosine'])\nprint(\"Predictions for 100 test Images:-\\n\",predictions_P)\naccuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\nprint(\"Accuracy on IPanda50 data: {:.2f}%\".format(accuracy_P * 100))\nprecision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\nrecall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\nf1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\nprint(\"Precision:\", precision_P)\nprint(\"Recall:\", recall_P)\nprint(\"F1 Score:\", f1_P)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **ON New Unseen Datasets**","metadata":{}},{"cell_type":"markdown","source":"**DogFaceNet Dataset**","metadata":{}},{"cell_type":"code","source":"# Download dataset (if not already downloaded)\ndatasets.DogFaceNet.get_data('../data/DogFaceNet')\n\n# Load dataset metadata\nmetadata = datasets.DogFaceNet('../data/DogFaceNet')\ntransform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\ndataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n\n\n# Download MegaDescriptor-T backbone from HuggingFace Hub\nbackbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n\n# Arcface loss - needs backbone output size and number of classes.\nobjective = TripletLoss()\n\n# Optimize parameters in backbone and in objective using single optimizer.\nparams = itertools.chain(backbone.parameters(), objective.parameters())\noptimizer = SGD(params=params, lr=0.001, momentum=0.9)\n\ndef print_epoch_loss(trainer, epoch_data):\n    # This function will print the average loss at the end of each epoch\n    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n\n\ntrainer = BasicTrainer(\n    dataset=dataset,\n    model=backbone,\n    objective=objective,\n    optimizer=optimizer,\n    epochs=20,\n    device='cuda',\n    epoch_callback=print_epoch_loss\n)\n\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n\n# trainer.save(\"retrained_chks\", file_name=\"tripletloss_DogFaceNet_retrained_checkpoint.pth\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\ndataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)\n\n# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\nextractor_P = DeepFeatures(trainer.model , device = 'cuda')\n\nquery_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity_function = CosineSimilarity()\nsimilarity_P = similarity_function(query_P, database_P)\nprint(similarity_P)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\npredictions_P = classifier_P(similarity_P['cosine'])\nprint(\"Predictions for 100 test Images:-\\n\",predictions_P)\naccuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\nprint(\"Accuracy on DogFaceNet data: {:.2f}%\".format(accuracy_P * 100))\nprecision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\nrecall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\nf1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\nprint(\"Precision:\", precision_P)\nprint(\"Recall:\", recall_P)\nprint(\"F1 Score:\", f1_P)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}