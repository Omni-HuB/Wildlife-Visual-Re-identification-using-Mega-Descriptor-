{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["%pip install wildlife-datasets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install wildlife-tools"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install timm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import timm\n","import numpy as np\n","from wildlife_datasets.datasets import MacaqueFaces\n","from wildlife_tools.data import WildlifeDataset\n","import torchvision.transforms as T\n","from wildlife_datasets import datasets, splits\n","from wildlife_tools.features import DeepFeatures\n","from wildlife_tools.similarity import CosineSimilarity\n","from wildlife_tools.inference import KnnClassifier\n","from sklearn.metrics import precision_score, recall_score, f1_score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import itertools\n","from itertools import chain\n","import timm\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torchvision.transforms as T\n","from torch.optim import SGD\n","\n","from wildlife_datasets.datasets import MacaqueFaces\n","from wildlife_tools.data import WildlifeDataset\n","\n","from wildlife_datasets import datasets, splits\n","from wildlife_tools.features import DeepFeatures\n","from wildlife_tools.similarity import CosineSimilarity\n","from wildlife_tools.inference import KnnClassifier\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","from wildlife_tools.data import WildlifeDataset, SplitMetadata\n","from wildlife_tools.train import ArcFaceLoss, BasicTrainer, TripletLoss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["**IPanda50 Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download dataset (if not already downloaded)\n","datasets.IPanda50.get_data('../data/IPanda50')\n","# Load dataset metadata\n","metadata = datasets.IPanda50('../data/IPanda50')\n","transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n","dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n","\n","\n","# Download MegaDescriptor-T backbone from HuggingFace Hub\n","backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n","\n","# Arcface loss - needs backbone output size and number of classes.\n","objective = ArcFaceLoss(\n","    num_classes=dataset.num_classes,\n","    embedding_size=768,\n","    margin=0.5,\n","    scale=64\n","    )\n","\n","# Optimize parameters in backbone and in objective using single optimizer.\n","params = itertools.chain(backbone.parameters(), objective.parameters())\n","optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n","\n","def print_epoch_loss(trainer, epoch_data):\n","    # This function will print the average loss at the end of each epoch\n","    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n","\n","\n","trainer = BasicTrainer(\n","    dataset=dataset,\n","    model=backbone,\n","    objective=objective,\n","    optimizer=optimizer,\n","    epochs=20,\n","    device='cuda',\n","    epoch_callback=print_epoch_loss\n",")\n","\n","trainer.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","trainer.save(\"retrained_chks\", file_name=\"arcfaceloass_IPanda50_retrained_checkpoint.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n","dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n","extractor_P = DeepFeatures(backbone)\n","query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["similarity_function = CosineSimilarity()\n","similarity_P = similarity_function(query_P, database_P)\n","print(similarity_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n","predictions_P = classifier_P(similarity_P['cosine'])\n","print(\"Predictions for 100 test Images:-\\n\",predictions_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n","print(\"Accuracy on IPanda50 data: {:.2f}%\".format(accuracy_P * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","print(\"Precision:\", precision_P)\n","print(\"Recall:\", recall_P)\n","print(\"F1 Score:\", f1_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Assuming you need to use the model for further operations\n","# # Initialize the model architecture again as needed for loading\n","# # Make sure to initialize it exactly as you did for training\n","# model = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=False)\n","\n","# # # Create a BasicTrainer instance with the initialized model (other parameters should match the training setup)\n","# # trainer = BasicTrainer(\n","# #     dataset=None,  # Assuming no dataset is needed just for loading\n","# #     model=model,\n","# #     objective=None,  # Assuming no objective needed just for loading\n","# #     optimizer=None,  # Assuming no optimizer needed just for loading\n","# #     epochs=0,  # No training epochs needed for just loading\n","# #     device='cuda'  # Adjust as per your device configuration\n","# # )\n","\n","# trainer = BasicTrainer(\n","#     dataset=dataset,\n","#     model=backbone,\n","#     objective=objective,\n","#     optimizer=optimizer,\n","#     epochs=2,\n","#     device='cuda',\n","#     epoch_callback=print_epoch_loss\n","# )\n","# # Load the model\n","# trainer.load(\"retrained_chks/arcfaceloass_retrained_checkpoint.pth\")\n","\n","# # After loading, the model in the trainer instance is now the re-trained model\n","# # and is ready for use in further processing or inference\n"]},{"cell_type":"markdown","metadata":{},"source":["**MacaqueFaces Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download dataset (if not already downloaded)\n","datasets.MacaqueFaces.get_data('../data/MacaqueFaces')\n","# Load dataset metadata\n","metadata = datasets.MacaqueFaces('../data/MacaqueFaces')\n","transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n","dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n","\n","\n","# Download MegaDescriptor-T backbone from HuggingFace Hub\n","backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n","\n","# Arcface loss - needs backbone output size and number of classes.\n","objective = ArcFaceLoss(\n","    num_classes=dataset.num_classes,\n","    embedding_size=768,\n","    margin=0.5,\n","    scale=64\n","    )\n","\n","# Optimize parameters in backbone and in objective using single optimizer.\n","params = itertools.chain(backbone.parameters(), objective.parameters())\n","optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n","\n","def print_epoch_loss(trainer, epoch_data):\n","    # This function will print the average loss at the end of each epoch\n","    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n","\n","\n","trainer = BasicTrainer(\n","    dataset=dataset,\n","    model=backbone,\n","    objective=objective,\n","    optimizer=optimizer,\n","    epochs=20,\n","    device='cuda',\n","    epoch_callback=print_epoch_loss\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","trainer.save(\"retrained_chks\", file_name=\"arcfaceloass_MacaqueFaces_retrained_checkpoint.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n","dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)\n","\n","# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n","extractor_P = DeepFeatures(backbone)\n","query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["similarity_function = CosineSimilarity()\n","similarity_P = similarity_function(query_P, database_P)\n","print(similarity_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n","predictions_P = classifier_P(similarity_P['cosine'])\n","print(\"Predictions for 100 test Images:-\\n\",predictions_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n","print(\"Accuracy on MacaqueFaces data: {:.2f}%\".format(accuracy_P * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","print(\"Precision:\", precision_P)\n","print(\"Recall:\", recall_P)\n","print(\"F1 Score:\", f1_P)"]},{"cell_type":"markdown","metadata":{},"source":["**LionData Dateset**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download dataset (if not already downloaded)\n","datasets.LionData.get_data('../data/LionData')\n","# Load dataset metadata\n","metadata = datasets.LionData('../data/LionData')\n","transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n","dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n","\n","\n","# Download MegaDescriptor-T backbone from HuggingFace Hub\n","backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n","\n","# Arcface loss - needs backbone output size and number of classes.\n","objective = ArcFaceLoss(\n","    num_classes=dataset.num_classes,\n","    embedding_size=768,\n","    margin=0.5,\n","    scale=64\n","    )\n","\n","# Optimize parameters in backbone and in objective using single optimizer.\n","params = itertools.chain(backbone.parameters(), objective.parameters())\n","optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n","\n","def print_epoch_loss(trainer, epoch_data):\n","    # This function will print the average loss at the end of each epoch\n","    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n","\n","\n","trainer = BasicTrainer(\n","    dataset=dataset,\n","    model=backbone,\n","    objective=objective,\n","    optimizer=optimizer,\n","    epochs=20,\n","    device='cuda',\n","    epoch_callback=print_epoch_loss\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","trainer.save(\"retrained_chks\", file_name=\"arcfaceloass_LionData_retrained_checkpoint.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n","dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n","extractor_P = DeepFeatures(backbone)\n","query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["similarity_function = CosineSimilarity()\n","similarity_P = similarity_function(query_P, database_P)\n","print(similarity_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n","predictions_P = classifier_P(similarity_P['cosine'])\n","print(\"Predictions for 100 test Images:-\\n\",predictions_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n","print(\"Accuracy on LionData data: {:.2f}%\".format(accuracy_P * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","print(\"Precision:\", precision_P)\n","print(\"Recall:\", recall_P)\n","print(\"F1 Score:\", f1_P)"]},{"cell_type":"markdown","metadata":{},"source":["**NyalaDataSet**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download dataset (if not already downloaded)\n","datasets.NyalaData.get_data('../data/NyalaData')\n","# Load dataset metadata\n","metadata = datasets.NyalaData('../data/NyalaData')\n","transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n","dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n","\n","\n","# Download MegaDescriptor-T backbone from HuggingFace Hub\n","backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n","\n","# Arcface loss - needs backbone output size and number of classes.\n","objective = ArcFaceLoss(\n","    num_classes=dataset.num_classes,\n","    embedding_size=768,\n","    margin=0.5,\n","    scale=64\n","    )\n","\n","# Optimize parameters in backbone and in objective using single optimizer.\n","params = itertools.chain(backbone.parameters(), objective.parameters())\n","optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n","\n","def print_epoch_loss(trainer, epoch_data):\n","    # This function will print the average loss at the end of each epoch\n","    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n","\n","\n","trainer = BasicTrainer(\n","    dataset=dataset,\n","    model=backbone,\n","    objective=objective,\n","    optimizer=optimizer,\n","    epochs=20,\n","    device='cuda',\n","    epoch_callback=print_epoch_loss\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","trainer.save(\"retrained_chks\", file_name=\"arcfaceloass_NyalaData_retrained_checkpoint.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n","dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)\n","\n","# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n","extractor_P = DeepFeatures(backbone)\n","query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["similarity_function = CosineSimilarity()\n","similarity_P = similarity_function(query_P, database_P)\n","print(similarity_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n","predictions_P = classifier_P(similarity_P['cosine'])\n","print(\"Predictions for 100 test Images:-\\n\",predictions_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n","print(\"Accuracy on NyalaData data: {:.2f}%\".format(accuracy_P * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","print(\"Precision:\", precision_P)\n","print(\"Recall:\", recall_P)\n","print(\"F1 Score:\", f1_P)"]},{"cell_type":"markdown","metadata":{},"source":["**StripeSpotter Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download dataset (if not already downloaded)\n","datasets.StripeSpotter.get_data('../data/StripeSpotter')\n","# Load dataset metadata\n","metadata = datasets.StripeSpotter('../data/StripeSpotter')\n","transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n","dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n","\n","\n","# Download MegaDescriptor-T backbone from HuggingFace Hub\n","backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n","\n","# Arcface loss - needs backbone output size and number of classes.\n","objective = ArcFaceLoss(\n","    num_classes=dataset.num_classes,\n","    embedding_size=768,\n","    margin=0.5,\n","    scale=64\n","    )\n","\n","# Optimize parameters in backbone and in objective using single optimizer.\n","params = itertools.chain(backbone.parameters(), objective.parameters())\n","optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n","\n","def print_epoch_loss(trainer, epoch_data):\n","    # This function will print the average loss at the end of each epoch\n","    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n","\n","\n","trainer = BasicTrainer(\n","    dataset=dataset,\n","    model=backbone,\n","    objective=objective,\n","    optimizer=optimizer,\n","    epochs=20,\n","    device='cuda',\n","    epoch_callback=print_epoch_loss\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","trainer.save(\"retrained_chks\", file_name=\"arcfaceloass_StripeSpotter_retrained_checkpoint.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n","dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n","extractor_P = DeepFeatures(backbone)\n","query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["similarity_function = CosineSimilarity()\n","similarity_P = similarity_function(query_P, database_P)\n","print(similarity_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n","predictions_P = classifier_P(similarity_P['cosine'])\n","print(\"Predictions for 100 test Images:-\\n\",predictions_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n","print(\"Accuracy on StripeSpotter data: {:.2f}%\".format(accuracy_P * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","print(\"Precision:\", precision_P)\n","print(\"Recall:\", recall_P)\n","print(\"F1 Score:\", f1_P)"]},{"cell_type":"markdown","metadata":{},"source":["**CZoo Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download dataset (if not already downloaded)\n","datasets.Czoo.get_data('../data/Czoo')\n","# Load dataset metadata\n","metadata = datasets.Czoo('../data/Czoo')\n","transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n","dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n","\n","\n","# Download MegaDescriptor-T backbone from HuggingFace Hub\n","backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n","\n","# Arcface loss - needs backbone output size and number of classes.\n","objective = ArcFaceLoss(\n","    num_classes=dataset.num_classes,\n","    embedding_size=768,\n","    margin=0.5,\n","    scale=64\n","    )\n","\n","# Optimize parameters in backbone and in objective using single optimizer.\n","params = itertools.chain(backbone.parameters(), objective.parameters())\n","optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n","\n","def print_epoch_loss(trainer, epoch_data):\n","    # This function will print the average loss at the end of each epoch\n","    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n","\n","\n","trainer = BasicTrainer(\n","    dataset=dataset,\n","    model=backbone,\n","    objective=objective,\n","    optimizer=optimizer,\n","    epochs=20,\n","    device='cuda',\n","    epoch_callback=print_epoch_loss\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","trainer.save(\"retrained_chks\", file_name=\"arcfaceloass_CZoo_retrained_checkpoint.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n","dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)\n","\n","# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n","extractor_P = DeepFeatures(backbone)\n","query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["similarity_function = CosineSimilarity()\n","similarity_P = similarity_function(query_P, database_P)\n","print(similarity_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n","predictions_P = classifier_P(similarity_P['cosine'])\n","print(\"Predictions for 100 test Images:-\\n\",predictions_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n","print(\"Accuracy on Czoo data: {:.2f}%\".format(accuracy_P * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","print(\"Precision:\", precision_P)\n","print(\"Recall:\", recall_P)\n","print(\"F1 Score:\", f1_P)"]},{"cell_type":"markdown","metadata":{},"source":["**CowDataset**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download dataset (if not already downloaded)\n","datasets.CowDataset.get_data('../data/CowDataset')\n","# Load dataset metadata\n","metadata = datasets.CowDataset('../data/CowDataset')\n","transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n","dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n","\n","\n","# Download MegaDescriptor-T backbone from HuggingFace Hub\n","backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n","\n","# Arcface loss - needs backbone output size and number of classes.\n","objective = ArcFaceLoss(\n","    num_classes=dataset.num_classes,\n","    embedding_size=768,\n","    margin=0.5,\n","    scale=64\n","    )\n","\n","# Optimize parameters in backbone and in objective using single optimizer.\n","params = itertools.chain(backbone.parameters(), objective.parameters())\n","optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n","\n","def print_epoch_loss(trainer, epoch_data):\n","    # This function will print the average loss at the end of each epoch\n","    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n","\n","\n","trainer = BasicTrainer(\n","    dataset=dataset,\n","    model=backbone,\n","    objective=objective,\n","    optimizer=optimizer,\n","    epochs=20,\n","    device='cuda',\n","    epoch_callback=print_epoch_loss\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","trainer.save(\"retrained_chks\", file_name=\"arcfaceloass_CowDataset_retrained_checkpoint.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n","dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)\n","\n","# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n","extractor_P = DeepFeatures(backbone)\n","query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["similarity_function = CosineSimilarity()\n","similarity_P = similarity_function(query_P, database_P)\n","print(similarity_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n","predictions_P = classifier_P(similarity_P['cosine'])\n","print(\"Predictions for 100 test Images:-\\n\",predictions_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n","print(\"Accuracy on CowDataset data: {:.2f}%\".format(accuracy_P * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","print(\"Precision:\", precision_P)\n","print(\"Recall:\", recall_P)\n","print(\"F1 Score:\", f1_P)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":[" # **On New Unseen Dataset**"]},{"cell_type":"markdown","metadata":{},"source":["**DogFaceNet Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Download dataset (if not already downloaded)\n","datasets.DogFaceNet.get_data('../data/DogFaceNet')\n","\n","# Load dataset metadata\n","metadata = datasets.DogFaceNet('../data/DogFaceNet')\n","transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n","dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n","\n","\n","# Download MegaDescriptor-T backbone from HuggingFace Hub\n","backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n","\n","# Arcface loss - needs backbone output size and number of classes.\n","objective = ArcFaceLoss(\n","    num_classes=dataset.num_classes,\n","    embedding_size=768,\n","    margin=0.5,\n","    scale=64\n","    )\n","\n","# Optimize parameters in backbone and in objective using single optimizer.\n","params = itertools.chain(backbone.parameters(), objective.parameters())\n","optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n","\n","def print_epoch_loss(trainer, epoch_data):\n","    # This function will print the average loss at the end of each epoch\n","    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n","\n","\n","trainer = BasicTrainer(\n","    dataset=dataset,\n","    model=backbone,\n","    objective=objective,\n","    optimizer=optimizer,\n","    epochs=20,\n","    device='cuda',\n","    epoch_callback=print_epoch_loss\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","trainer.save(\"retrained_chks\", file_name=\"arcfaceloass_DogFaceNet_retrained_checkpoint.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n","dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n","extractor_P = DeepFeatures(trainer.model)\n","query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["similarity_function = CosineSimilarity()\n","similarity_P = similarity_function(query_P, database_P)\n","print(similarity_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n","predictions_P = classifier_P(similarity_P['cosine'])\n","print(\"Predictions for 100 test Images:-\\n\",predictions_P)\n","accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n","print(\"Accuracy on IPanda50 data: {:.2f}%\".format(accuracy_P * 100))\n","precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n","print(\"Precision:\", precision_P)\n","print(\"Recall:\", recall_P)\n","print(\"F1 Score:\", f1_P)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
