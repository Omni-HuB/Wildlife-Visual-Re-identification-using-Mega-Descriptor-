{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install wildlife-datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install wildlife-tools","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install timm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import timm\nimport numpy as np\nfrom wildlife_datasets.datasets import MacaqueFaces\nfrom wildlife_tools.data import WildlifeDataset\nimport torchvision.transforms as T\nfrom wildlife_datasets import datasets, splits\nfrom wildlife_tools.features import DeepFeatures\nfrom wildlife_tools.similarity import CosineSimilarity\nfrom wildlife_tools.inference import KnnClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\nfrom itertools import chain\nimport timm\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom torch.optim import SGD\n\nfrom wildlife_datasets.datasets import MacaqueFaces\nfrom wildlife_tools.data import WildlifeDataset\n\nfrom wildlife_datasets import datasets, splits\nfrom wildlife_tools.features import DeepFeatures\nfrom wildlife_tools.similarity import CosineSimilarity\nfrom wildlife_tools.inference import KnnClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nfrom wildlife_tools.data import WildlifeDataset, SplitMetadata\nfrom wildlife_tools.train import ArcFaceLoss, BasicTrainer, TripletLoss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**IPanda50 Dataset**","metadata":{}},{"cell_type":"code","source":"# Download dataset (if not already downloaded)\ndatasets.IPanda50.get_data('../data/IPanda50')\n# Load dataset metadata\nmetadata = datasets.IPanda50('../data/IPanda50')\ntransform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\ndataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n\n\n# Download MegaDescriptor-T backbone from HuggingFace Hub\nbackbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n\n# Arcface loss - needs backbone output size and number of classes.\nobjective = ArcFaceLoss(\n    num_classes=dataset.num_classes,\n    embedding_size=768,\n    margin=0.5,\n    scale=64\n    )\n\n# Optimize parameters in backbone and in objective using single optimizer.\nparams = itertools.chain(backbone.parameters(), objective.parameters())\noptimizer = SGD(params=params, lr=0.001, momentum=0.9)\n\ndef print_epoch_loss(trainer, epoch_data):\n    # This function will print the average loss at the end of each epoch\n    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n\n\ntrainer = BasicTrainer(\n    dataset=dataset,\n    model=backbone,\n    objective=objective,\n    optimizer=optimizer,\n    epochs=20,\n    device='cuda',\n    epoch_callback=print_epoch_loss\n)\n\ntrainer.train()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\ndataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\nextractor_P = DeepFeatures(backbone)\nquery_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity_function = CosineSimilarity()\nsimilarity_P = similarity_function(query_P, database_P)\nprint(similarity_P)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\npredictions_P = classifier_P(similarity_P['cosine'])\nprint(\"Predictions for 100 test Images:-\\n\",predictions_P)\n\naccuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\nprint(\"Accuracy on IPanda50 data: {:.2f}%\".format(accuracy_P * 100))\n\nprecision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\nrecall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\nf1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\nprint(\"Precision:\", precision_P)\nprint(\"Recall:\", recall_P)\nprint(\"F1 Score:\", f1_P)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntrainer.save(\"retrained_chks\", file_name=\"arcfaceloass_IPanda50_retrained_checkpoint.pth\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Assuming you need to use the model for further operations\n# # Initialize the model architecture again as needed for loading\n# # Make sure to initialize it exactly as you did for training\n# model = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=False)\n\n# # # Create a BasicTrainer instance with the initialized model (other parameters should match the training setup)\n# # trainer = BasicTrainer(\n# #     dataset=None,  # Assuming no dataset is needed just for loading\n# #     model=model,\n# #     objective=None,  # Assuming no objective needed just for loading\n# #     optimizer=None,  # Assuming no optimizer needed just for loading\n# #     epochs=0,  # No training epochs needed for just loading\n# #     device='cuda'  # Adjust as per your device configuration\n# # )\n\n# trainer = BasicTrainer(\n#     dataset=dataset,\n#     model=backbone,\n#     objective=objective,\n#     optimizer=optimizer,\n#     epochs=2,\n#     device='cuda',\n#     epoch_callback=print_epoch_loss\n# )\n# # Load the model\n# trainer.load(\"retrained_chks/arcfaceloass_retrained_checkpoint.pth\")\n\n# # After loading, the model in the trainer instance is now the re-trained model\n# # and is ready for use in further processing or inference\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # **On New Unseen Dataset**","metadata":{}},{"cell_type":"markdown","source":"**DogFaceNet Dataset**","metadata":{}},{"cell_type":"code","source":"\n# Download dataset (if not already downloaded)\ndatasets.DogFaceNet.get_data('../data/DogFaceNet')\n\n# Load dataset metadata\nmetadata = datasets.DogFaceNet('../data/DogFaceNet')\ntransform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\ndataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n\n\n# Download MegaDescriptor-T backbone from HuggingFace Hub\nbackbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n\n# Arcface loss - needs backbone output size and number of classes.\nobjective = ArcFaceLoss(\n    num_classes=dataset.num_classes,\n    embedding_size=768,\n    margin=0.5,\n    scale=64\n    )\n\n# Optimize parameters in backbone and in objective using single optimizer.\nparams = itertools.chain(backbone.parameters(), objective.parameters())\noptimizer = SGD(params=params, lr=0.001, momentum=0.9)\n\ndef print_epoch_loss(trainer, epoch_data):\n    # This function will print the average loss at the end of each epoch\n    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n\n\ntrainer = BasicTrainer(\n    dataset=dataset,\n    model=backbone,\n    objective=objective,\n    optimizer=optimizer,\n    epochs=20,\n    device='cuda',\n    epoch_callback=print_epoch_loss\n)\n\ntrainer.train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntrainer.save(\"retrained_chks\", file_name=\"arcfaceloass_DogFaceNet_retrained_checkpoint.pth\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\ndataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\nextractor_P = DeepFeatures(trainer.model)\nquery_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity_function = CosineSimilarity()\nsimilarity_P = similarity_function(query_P, database_P)\nprint(similarity_P)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\npredictions_P = classifier_P(similarity_P['cosine'])\nprint(\"Predictions for 100 test Images:-\\n\",predictions_P)\naccuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\nprint(\"Accuracy on IPanda50 data: {:.2f}%\".format(accuracy_P * 100))\nprecision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\nrecall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\nf1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\nprint(\"Precision:\", precision_P)\nprint(\"Recall:\", recall_P)\nprint(\"F1 Score:\", f1_P)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}